---
title: "wCorr Arguments"
author: "Paul Bailey, Ahmad Emad, Ting Zhang, Qingshu Xie"
date: "`r Sys.Date()`"
output: pdf_document
vignette: >
  %\VignetteIndexEntry{wCorr Arguments}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r packages and data, echo=FALSE, results="hide", message=FALSE,warning=FALSE}
require(knitr)
require(wCorr)
require(lattice)
require(doBy)
load("../vignettes/sim/ML.RData")
load("../vignettes/sim/fast.RData")
load("../vignettes/sim/speed.RData")
```

The wCorr package can be used to calculate Pearson, Spearman, polyserial, and polychoric correlations, in weighted or unweighted form.^[The estimation procedure used by the wCorr package for the polyserial is based on the likelihood function in by Cox, N. R. (1974), "Estimation of the Correlation between a Continuous and a Discrete Variable." *Biometrics*, **30** (1), pp 171-178. The likelihood function for polychoric is from Olsson, U. (1979) "Maximum Likelihood Estimation of the Polychoric Correlation Coefficient." *Psyhometrika*, **44** (4), pp 443-460. The likelihood used for Pearson and Spearman is written down many places. One is the "correlate" function in Stata Corp, Stata Statistical Software: Release 8. College Station, TX: Stata Corp LP, 2003.] The package implements the tetrachoric correlation as a specific case of the polychoric correlation and biserial correlation as a specific case of the polyserial correlation. When weights are used, the correlation coefficients are calculated with so called sample weights or inverse probability weights.^[Sample weights are comparable to `pweight` in Stata.]

This vignette describes the use of applying different options for computation and impact on resulting correlations of two Boolean switches in the wCorr package. First, the Maximum Likelihood, or `ML` switch uses   either a consistent but non-Maximum-Likelihood Estimator (MLE) for the nuisance parameters that define the binning process to be used (`ML=FALSE`) or for the nuisance parameters to be estimated using the MLE (`ML=TRUE`). Second the `fast` argument gives the option to use a pure R implementation (`fast=FALSE`) or an implementation that relies on the `Rcpp` and `RcppArmadillo` packages (`fast=TRUE`).

The results show that setting the `fast` argument to `TRUE` changes results by less than $10^{-8}$ but can speed up computation for large data sets, especially for the polychoric correlation where a speed up hundreds of seconds to seconds can be achieved. The `ML` option speeds up both computations--polychoric most noticably for small $n$ size and polyserial most noticably for large $n$ size. The difference between the two estimates can be as large as in the second or third digit of a correlation coefficient when $n=10$ but is noticable only in the fourth digit once $n=1,000$.

The *wCorr Formulas* vignette describes the statistical properties of the correlation estimators in the package and has a more complete derivation of the likelihood functions.

# The `ML` switch
The wCorr package computes correlation coefficients between two vectors of random variables that are jointly bivariate normal. We call the two vectors ***X*** and ***Y***.

$$\begin{pmatrix} X \\ Y \end{pmatrix} \sim N \left[ \begin{pmatrix} \mu_x \\ \mu_y \end{pmatrix}, \boldsymbol{\Sigma} \right] $$

where $N(\mathbf{A},\boldsymbol{\Sigma})$ is the bivariate normal distribution with mean ***A*** and covariance $\boldsymbol{\Sigma}$.

## Computation of polyserial correlation

The likelihood function for an individual observation of the polyserial correlation is^[See the *wCorr Formulas* vignette for a more complete description and motivation for the polyserial correlations's likelihood function.]

$$\mathrm{Pr}\left( \rho=r, \boldsymbol{\theta} ; Z=z_i, M=m_i \right) = \phi(z_i) \left[ \Phi\left( \frac{\theta_{m_i+2} - r \cdot z_i}{\sqrt{1-r^2}} \right) - \Phi \left( \frac{\theta_{m_i+1} - r \cdot z_i}{\sqrt{1-r^2}} \right) \right]$$

where $\rho$ is the correlation between ***X*** and ***Y***, ***Z*** is the normalized version of ***X***, and ***M*** is a discretized version of ***Y***, using $\boldsymbol{\theta}$ as cut points as described in the *wCorr Formulas* vignette.

The log-likelihood is then

$$\ell(\rho, \boldsymbol{\theta};z,m) = \sum_i w_i \ln\left[ \mathrm{Pr}\left( \rho=r, \boldsymbol{\theta} ; Z=z_i, M=m_i \right) \right]$$

The derivatives of $\ell$ can be written down but are not readily computed. When the `ML` argumet is set to `FALSE` (the default), a one dimensional optimization of $\rho$ is calculated using `stats::optimize`. When the `ML` argument is set to `TRUE`, a multi-dimensional optimization is done for $\rho$ and $\boldsymbol{\theta}$ using `minqa::bobyqa`.

## Computation of polychoric correlation  
For the polychoric the observed data is discreteized for both variables. Here the discretized version of ***X*** is ***P*** and the discretized version of ***Y*** remains ***M***.^[See the "wCorr Formulas" vignette for a more complete description and motivation for the polychoric correlations's likelihood function.] The likelihood function for the polychoric is

$$\mathrm{Pr}\left( \rho=r, \boldsymbol{\theta}, \boldsymbol{\theta}' ; P=p_i, M=m_i \right) = \int_{\theta_{p_i+1}'}^{\theta_{p_i+2}'}  \int_{\theta_{m_i+1}}^{\theta_{m_i+2}} \mkern-40mu  f(x,y|\rho=r) dy dx$$

where $f(x,y|r)$ is the noramlzied bivariate normal distribution with correlation $\rho$,  $\boldsymbol{\theta}$  are the cut points used to discretize Y into M, and  $\boldsymbol{\theta'}$ are the cut points used to discretize ***X*** into ***P***.

The log-likelihood is then
$$\ell(\rho, \boldsymbol{\theta}, \boldsymbol{\theta}' ;\mathbf{p},\mathbf{m}) = \sum_i w_i \ln\left[\mathrm{Pr}\left( \rho=r, \boldsymbol{\theta}, \boldsymbol{\theta}' ; P=p_i, M=m_i \right) \right] $$

The derivatives of $\ell$ can be written down but are not readily computed. When the `ML` argumet is set to `FALSE` (the default), a one dimensional optimization of $\rho$ is calculated using `stats::optimize`. When the `ML` argument is set to `TRUE`, a multi-dimensional optimization is done for $\rho$, $\boldsymbol{\theta}$, and $\boldsymbol{\theta}'$ using `minqa::bobyqa`.

# General procedures of the simulation study of unweighted correlations

A simulation is run several times. For each itteration, the following procedure is used:

* select a true correlation coefficient $\rho$;
* select the number of observations ($n$);
* generate ***X*** and ***Y*** to be bivariate normally distributed using a pseudo-Random Number Generator (RNG);
* using a pseudo-RNG, select the the number of bins for ***M*** and ***P*** ($t$ and $t'$) independantly from the set \{2, 3, 4, 5\};
* select the bin boundaries for ***M*** and ***P*** ($\boldsymbol{\theta}$ and $\boldsymbol{\theta}'$) by sorting the results of $(t-1)$ and $(t'-1)$ draws, respectively, from a normal distribution using a pseudo-RNG; 
* confirm that at least 2 levels of each of ***M*** and ***P*** are occupied (if not, retrun to generating ***X*** and ***Y***); and
* calculate and record relevant statistics.

When the exact method of selecting a parameter (such as $n$) is not noted above, it is described as part of each simulation.

# ML switch

A simulation was done at each level of the cartesian product of  $\mathtt{ML} \in \{\mathtt{TRUE}, \mathtt{FALSE} \}$, \newline $\rho \in \left( -0.99, -0.95, -0.90, -0.85, ..., 0.95, 0.99 \right)$, and $n \in \{10, 100, 1000\}$. For precision, each iteration is run three times. The same values of the variables are used in the computation for `ML=TRUE` as well as for `ML=FALSE`; and then the statistics are compared between the two sets of results. Mean absolute difference (MAD) between the maximum likelihood estimator (`ML=TRUE`) and the estimator that uses a consistent estimate of $\boldsymbol{\theta}$ (and potentially $\boldsymbol{\theta}'$) and then maximizes over $\rho$ only (`ML=FALSE`).  It is given by

$$MAD= | r_{ML=TRUE} - r_{ML=FALSE} |  $$

where $r_{ML=TRUE}$ is the estimated correlation when `ML=TRUE`; and  $r_{ML=FALSE}$ is the estimated correlation when `ML=FALSE`. Thus, this MAD is not deviation from the true $\rho$ but the difference between the `ML` values.

This is a plot of the $MAD$ as a function of the true correlation coefficnet. It shows a decrease in MAD with the increase of $n$ increases (change from line to line), a decrease when the correlations are in the neighborhood of zero that is more pronounced for larger $n$, and a dip when the correlation is exactly 1 or -1.

```{r ML MAD plot, echo=FALSE,fig.width=6, fig.height=2.5}
ml <- subset(ML, type %in% c("Polychoric", "Polyserial"))
ml$i <- rep(1:(nrow(ml)/2),each=2)
mml <- merge(subset(ml,ML),
               subset(ml,!ML, c("i", "est")),
               by="i",
               suffixes=c(".ml",".nonml"))
mml$ML <- NULL
mml$rmsedrho <- (mml$est.ml - mml$est.nonml)^2
mml$drho <- mml$est.ml - mml$est.nonml
mml$absdrho <- abs(mml$est.ml - mml$est.nonml)

agg <- summaryBy(absdrho ~ n + rho + type, data=mml, FUN=mean, na.rm=TRUE)
xyplot(absdrho.mean ~ rho|type,
       data=agg,
       groups=n,
       scales=list(y=list(log=10, cex=0.7), x = list(cex=0.7)),
       type="l",
       ylab="MAD",
       xlab=expression(rho),
       auto.key=list(lines=TRUE, points=FALSE, space="right", cex=0.7),
       par.settings=list(superpose.line=list(lwd=2), plot.line=list(lwd=2)))
```

This table shows the $MAD$ by $n$ and correlation type.
```{r ML MAD table, echo=FALSE}
agg <- summaryBy(absdrho ~ type+n, data=mml, FUN=mean, na.rm=TRUE)
colnames(agg) <- c("Correlation type", "n", "MAD")
kable(agg)
```

These resuls show that the size of the MAD decreases as $n$ incrreases. When the sample size is $n=10$, the MAD is less than 0.01 for the polyserial and 0.004 for the polychoric. 

# fast switch

This section examines the agreement between the pure R implementation of the optimizations and the `Rcpp` and `RcppArmadillo` impelemntation. The code can compute with either option by setting `fast=FALSE` (pure R) or `fast=TRUE` (Rcpp).

A simulation was done at each level of the cartesian product of  $\mathtt{fast} \in \{\mathtt{TRUE}, \mathtt{FALSE} \}$, \newline $\rho \in \left( -0.99, -0.95, -0.90, -0.85, ..., 0.95, 0.99 \right)$, and $n \in \{10, 100, 1000\}$. Each iteration was run 100 times. The same values of the variables are used  in the computation for `fast=TRUE` as well as for  `fast=FALSE`; and then the statistics are compared between the two sets of results.

The plot below shows all differences between the `fast=TRUE` and `fast=FALSE runs` for the four types of correlations. Note that differences smaller than $10^{-16}$ are indistinguishable from 0 by the machine. However, a factor of $10^{-17}$ was added to the results so that they could all be shown on a log scale. 

```{r fast MAD plot, echo=FALSE,fig.width=6, fig.height=3.5}
fast$i <- rep(1:(nrow(fast)/2),each=2)
mfast <- merge(subset(fast,fast),
               subset(fast,!fast, c("i", "est")),
               by="i",
               suffixes=c(".fast",".slow"))
mfast$fast <- NULL
mfast$drho <- mfast$est.fast - mfast$est.slow
mfast$absdrho <- abs(mfast$est.fast - mfast$est.slow) + 1E-17

agg <- summaryBy(absdrho ~ n + rho + type, data=mfast, FUN=mean, na.rm=TRUE)
xyplot(absdrho.mean ~ rho|type,
       data=agg,
       groups=n,
       type="l",
       ylab="MAD",
       scales=list(y=list(log=10, cex=0.7), x=list(cex=0.7)),
       xlab=expression(rho),
       auto.key=list(lines=TRUE, points=FALSE, space="right", cex=0.7),
       par.settings=list(superpose.line=list(lwd=2), plot.line=list(lwd=2))
       )
```

The above shows that differences as a result of the `fast` argument are never larger than $10^{-8}$ for any type. The Spearman never shows any difference that is different from zero and the Pearson show differences just larger than the smallest observable difference when using double precision floating point values (about $1 \times 10^{-16}$.

# Implications for speed

A simulation was done at each level of the cartesian product of  $\mathtt{ML} \in \{\mathtt{TRUE}, \mathtt{FALSE} \}$, $\mathtt{fast} \in \{\mathtt{TRUE}, \mathtt{FALSE} \}$, $\rho \in \left( -0.99, -0.95, -0.90, -0.85, ..., 0.95, 0.99 \right)$, and $n \in \{10^1, 10^{0.75}, 10^{1.5}, ..., 10^7\}$. For precision, each iteration is run 80 times when $n<10^5$ and 20 times when $n\geq 10^5$. The same values of the variables are used in the computations at all four combinations of `ML` and `fast`. A variety of correlations are chosen so that the results represent an average of possible values of $\rho$.

The following plot shows the mean computing time versus $n$.

```{r plot speed, echo=FALSE,fig.width=6, fig.height=3.5}
speed$class <- ifelse(speed$ML, "ML=T,", "ML=F,")
speed$class <- paste0(speed$class, ifelse(speed$fast, "fast=T", "fast=F"))
speed$t <- pmax(speed$t, 0.001)
agg <- summaryBy(t ~ n + type + class, data=speed, FUN=mean, na.rm=TRUE)
xyplot(t.mean ~ n|type,
       data=agg,
       type="l",
       ylab="Computing Time",
       scales=list(y=list(log=10, cex=0.7), x=list(log=10, cex=0.7)),
       xlab="n",
       groups=class,
       auto.key=list(lines=TRUE, points=FALSE, space="right", cex=0.7),
       par.settings=list(superpose.line=list(lwd=2), plot.line=list(lwd=2))
       )
```

#Conclusion
Using tables presented in this vignette, users who wish to use the more accurate `ML=TRUE` argument can compare the difference in computing time and the difference in results.

The `fast` argument is provided primarily for comparison of the `Rcpp` and pure R code and shows agreement to within $10^{-8}$.
