---
title: "wCorr Formulas"
author: "Paul Bailey, Ahmad Emad, (people who do QC for this product)"
date: '`r Sys.Date()`'
output: pdf_document
vignette: |
  %\VignetteIndexEntry{wCorr Formulas} %\VignetteEngine{knitr::rmarkdown} %\VignetteEncoding{UTF-8}
---

```{r packages and data, echo=FALSE, results="hide", message=FALSE, warning=FALSE}
require(wCorr)
require(lattice)
require(doBy)
load("//dc2fs/DC4Work/ESSIN Task 14/NAEP R Program/Paul/wCorr/wCorr/vignettes/ntime.RData")
load("//dc2fs/DC4Work/ESSIN Task 14/NAEP R Program/Paul/wCorr/wCorr/vignettes/bias.RData")
load("//dc2fs/DC4Work/ESSIN Task 14/NAEP R Program/Paul/wCorr/wCorr/vignettes/wgtvrho.RData")
load("//dc2fs/DC4Work/ESSIN Task 14/NAEP R Program/Paul/wCorr/wCorr/vignettes/wgtvn.RData")
```

This document introduces the methodology used in the wCorr package for computing the weighted Pearson, Spearman, polyserial, and polychoric correlations. For the polyserial and polychoric correlations, the coefficient is estimated using a numerical likelihood maximization. The weighted likelihood functions that are used are motivated and described.

For the polyserial and polychoric correlations, evidence is offered for the correctness of the methods, including an examination of the bias and consistency of both methods and numerical simulations to show:

* the bias of the methods as a function of the true correlation coefficient ($\rho$) and the number of observations ($n$); and
* the root mean squared error (RMSE) of the methods as a function of $\rho$ and $n$ in the unweighted and weighed cases.

The *wCorr Arguments* vignette describes the effects the `ML` and `fast` arguments have on computation.

# Methodology
Here we focus on measurement of the correlation coefficients between two vectors of random variables that are jointly bivariate normal. We call the two vectors ***X*** and ***Y***.^[The Spearman correlation coefficient can be motivated by a much more broad class of random variables but that is not dealt with in this document.] The $i^{th}$ members of the vectors are then called $x_i$ and $y_i$.

##Methodology for Pearson and Spearman correlations 

The Pearson correlation is computed using the formula
$$\rho_{Pearson}=\frac{\sum_i \left[ w_i (x_i-\bar{x})(y_i-\bar{y}) \right]}{\sqrt{\sum_i \left( w_i  (x-\bar{x})^2 \right)\sum_i \left( w_i  (y-\bar{y})^2 \right) }} $$

where $\bar{x}=\frac{1}{\sum w_i}\sum_i w_i x_i$, $\bar{y}=\frac{1}{\sum w_i}\sum_i w_i y_i$, and $n$ is the number of elements in ***X*** and ***Y***.^[See the "correlate" function in Stata Corp, Stata Statistical Software: Release 8. College Station, TX: Stata Corp LP, 2003.]

The Spearman correlation coefficient is calculated with the rank scores of the data  that is obtained before the same formula is applied. When data are ranked, ties must be handled in some way. The chosen method is to assign the average of all tied ranks. For example, if the second and third rank units are tied then both units would receive a rank of 2.5 (the average of 2 and 3).

##Methodology for polyserial correlation
For the polyserial correlation, it is again assumed that there are two continuous variables ***X*** and ***Y*** that have a bivariate normal distribution.^[For a more complete treatment of the polyserial correlation, see Cox, N. R., "Estimation of the Correlation between a Continuous and a Discrete Variable" *Biometrics*, **50** (March), 171-187, 1974.]

$$\begin{pmatrix} X \\ Y \end{pmatrix} \sim N \left[ \begin{pmatrix} \mu_x \\ \mu_y \end{pmatrix}, \Sigma \right] $$

Where $N(a, \Sigma)$ is a bivariate normal distribution with mean $a$ and variance $\Sigma$. For the polyserial correlation, ***Y*** is discretized into the random variable ***M*** according to

$$m_i= \begin{cases} 1 \quad \mathrm{if}  \theta_2 < y_i < \theta_3 \\ 2 \quad \mathrm{if} \theta_3 < y_i < \theta_4 \\ \vdots \\ t \quad \mathrm{if} \theta_{t+1} < y_i < \theta_{t+2} \end{cases}$$

where, for notational convenience, $\theta_2 \equiv -\infty$ and $\theta_{t+2} \equiv \infty$.^[The indexing is somewhat odd to be consistent with Cox (1974). Nevertheless, this treatment does not use the Cox definition of $\theta_0$, $\theta_1$ or $\theta_2$ which are either not estimated (as is the case for $\theta_0$, and $\theta_1$) or are reappopriated (as is the case for $\theta_2$). Cox calls the correlation coefficient $\theta_2$ while this document uses $\rho$ and uses $\theta_2$ to store $-\infty$ as a convenience so that the vector $\boldsymbol{\theta}$ includes the (infinite) bounds as well as the interior points.]

The following figure shows the density of ***Y*** when $\theta=\left(-\infty,-2,-0.5,1.6,\infty\right)$. Here, for example, any value of $-2 < y_i < -0.5$ would have $m_i=2$.

```{r theta,echo=FALSE,results="hide",fig.show="hold",fig.width=6, fig.height=2.5}
x <- seq(-3,3,by=0.01) 
y <- dnorm(x)
par0 <- par(no.readonly=TRUE)
par(ann=FALSE)
par(mar=c(5,2,1,1)+0.1)
plot(x,y,type="l",xlab="y",ylab="density", xaxt="n", yaxt="n")
axis(1,at=c(-2,-0.5,1.6), labels=expression(theta[3],theta[4],theta[5]))
text(x=c(-2.5,-1.25,0.55,2.3),y=0.05, labels=paste0("m=",1:4))
theta <- c(-2,-0.5,1.6)
for(i in 1:3) {
  lines(rep(theta[i],2), c(-1,dnorm(theta[i])))
}
par(ann=TRUE)
par(mgp=c(0.5,0,0))
title(ylab="density")
par(mgp=c(3,1,0))
title(xlab="Y")
par(par0)
```

Notice that $\mu_y$ is not identifed (or is irrelevant) because setting $\tilde{\mu}_y = \mu_y + a$ and $\tilde{\boldsymbol{\theta}}=\boldsymbol{\theta} + a$ lead to exactly the same values of $\mathbf{M}$ and so one of the two must be arbitrarily assigned. A convenient decision is to decide $\mu_y \equiv 0$. A similar argument holds for $\sigma_y$ so that  $\sigma_y \equiv 1$.

For ***X***, Cox (1974) observes that the MLE mean and standard deviation of ***X*** are simply the average and (population) standard deviation of the data and do not depend on the other parameters.^[The population standard devaition is used because it is the MLE for the standard deviation. Notice that, while the sample variance is an unbiased estimator of the variance and the population variance is not an unbaised estimator of the variance, they are very similar and the variance is also a nuisance parameter, not a parameter of interest when finding the correlation.] This can be taken advantage of by defining $z \equiv \frac{x- \bar{x}}{ \hat\sigma_x}$. 

Combining these simplifications, the probability of any given $x_i$, $m_i$ pair is

$$\mathrm{Pr}\left( \rho=r ; Z=z_i, M=m_i \right) = \phi(z_i) \int_{\theta_{m_i+1}}^{\theta_{m_i+2}} \mkern-40mu  f(y|Z=z,\rho=r)dy$$

where $\mathrm{Pr}\left( \rho=r ; Z=z, M=m \right)$ is the probability of the event $\rho=r$ given the data $z$ and $m$, $\phi(\cdot)$ is the standard normal and $f(y|z,\rho)$ is the distribution of $y$ conditional on $z$ and $rho$. Because $y$ and $z$ are jointly normally distributed (by assumption)
$$f(y|Z=z,\rho=r) =N\left(\mu_y + \frac{\sigma_y}{\sigma_z}r(z-\mu_z), (1-r^2){\sigma_y}^2 \right)$$
because both ***Z*** and ***Y*** are standard normals
$$f(y|Z=z,\rho=r) =N\left(r \cdot z, (1-r^2) \right)$$
now, define $w \equiv \frac{y-r\cdot z}{\sqrt{1-r^2}}$ and $w$ has a standard normal distribution. Plugging this in
$$\mathrm{Pr}\left( \rho=r, \theta ; Z=z_i, M=m_i \right) = \phi(z_i) \left[ \Phi\left( \frac{\theta_{m_i+2} - r \cdot z_i}{\sqrt{1-r^2}} \right) - \Phi \left( \frac{\theta_{m_i+1} - r \cdot z_i}{\sqrt{1-r^2}} \right) \right]$$

Where $\Phi(\cdot)$ is the standard normal cumulative density function. Using the above probability function as an objective, the log-likelihood is then maximized.
$$\ell(\rho;z,m) = \sum_i w_i \ln\left[ \mathrm{Pr}\left( \rho=r ; Z=z_i, M=m_i \right) \right]$$
where $w_i$ is the weight of the $i^{th}$ members of the vectors Z and Y.

The value of the nuisance paramter $\boldsymbol{\theta}$ is chosen to be $\Phi^{-1}(n/N)$ where $n$ is the number of values to the left of the cut point ($\theta_i$ value) and $N$ is the number of data points overall.

###Computation of polyserial correlation
The derivatives of $\ell$ can be computed but are not readily computed. When the `ML` argumet is set to `FALSE` (the default), a one dimensional optimization of $\rho$ is calculated using `stats::optimize`. When the `ML` argument is set to `TRUE`, a multi-dimensional optimization is done for $\rho$ and $\boldsymbol{\theta}$ using `minqa::bobyqa`. As is shown below, the difference between these two is slight, if present, and so the default value of `ML` is recommended.

Because the optimization is not perfect when the correlation is in a boundary condition ($\rho \in \{-1,1\}$), a check for perfect correlation  is performed before the above optimization by simply examining if the values of ***X*** and ***M*** have exactly the same order.

##Methodology for polychoric correlation

Similar to the polyserial correlation, the polychoric correlation is a simple case of two continuous variables ***X*** and ***Y*** that have a bivariate normal distribution. In the case of the polyserial correlation the continuous (latent) variable ***Y*** was observed as a discretized variable ***M***. For the polychoric correlation, this is again true but now the continuous (latent) variable ***X*** is observed as a discrete variable ***P*** according to

$$p_i= \begin{cases} 1 \quad \mathrm{if}  \theta'_2 < x_i < \theta'_3 \\ 2 \quad \mathrm{if} \theta'_3 < x_i < \theta'_4 \\ \vdots \\ t \quad \mathrm{if} \theta'_{t+1} < x_i < \theta'_{t+2} \end{cases}$$

where $\boldsymbol{\theta}$ remains the cut points for the distibution defining the transformation of ***Y*** to ***M*** and $\boldsymbol{\theta}'$ is the cut points for the transformation from ***X*** to ***P***. Similar to $\boldsymbol{\theta}$, $\boldsymbol{\theta}'$ has $\theta_2' \equiv -\infty$ and $\theta_{t+2}' \equiv \infty$.

As in the polyserial correlation, $\mu_y$ is not identifed (or is irrelevant) because setting $\tilde{\mu}_y = \mu_y + a$ and $\tilde{\boldsymbol{\theta}}=\boldsymbol{\theta} + a$ lead to exactly the same values of $\mathbf{M}$ and so one of the two must be arbitrarily assigned. The same is true for $\mu_x$. A convenient decision is to decide $\mu_y = \mu_x \equiv 0$. A similar argument holds for $\sigma_y$ and $\sigma_x$ so that  $\sigma_y = \sigma_x \equiv 1$

Then the probability of any given $m_i$, $p_i$ pair is 

$$\mathrm{Pr}\left( \rho=r ; P=p_i, M=m_i \right) = \int_{\theta_{p_i+1}'}^{\theta_{p_i+2}}  \int_{\theta_{m_i+1}}^{\theta_{m_i+2}} \mkern-40mu  f(x,y|\rho=r)dydx$$

where $\rho$ is the correlation coefficient.

Using this function as an objective, the log-likelihood is then maximized.
$$\ell(\rho;p,m) = \sum_i w_i \ln\left[\mathrm{Pr}\left( \rho=r ; P=p_i, M=m_i \right) \right] $$

###Computation of polychoric correlation
This again mirrors the treatment of the polyserial. The derivatives of $\ell$ can be computed but are not readily computed. When the `ML` argument is set to `FALSE` (the default), a one dimensional optimization of $\rho$ is calculated using `stats::optimize`. When the `ML` argument is set to `TRUE` a multi-dimensional optimization is done for $\rho$, $\boldsymbol{\theta}$, and $\boldsymbol{\theta}'$ using `minqa::bobyqa`. As is shown below, the difference between these two is slight, if present, and so the default value of `ML` is recommended.

Because the optimization is not perfect when the correlation is in a boundary condition ($\rho \in \{-1,1\}$), a check for perfect correlation  is performed before the above optimization by simply examining if the values of ***P*** and ***M*** have a Goodman-Kruskal correlation coefficient of -1 or 1. When this is the case, the MLE of -1 or 1, respectively, is returned.

# Correctness of the estimating methods

It is easy to prove the consistency of the $\boldsymbol{\theta}$ for the polyserial correlation and $\boldsymbol{\theta}$ and $\boldsymbol{\theta}'$ for the polychoric correlation using the non-ML case. Similarly, for $\rho$, because it is an MLE that can be obtained by taking a derivative and setting it equal to zero, the results are asymtotically unbiased and obtain the Cramer-Rao lower bound.

This does not speak to the small sample properties of these correlation coefficients. Previous work has described their properties by simulation; and that tradition is continued below.

## Simulation study of unweighted correlations

A simulation is run several times. For each iteration, the following procedure is used:

* select the number of observations ($n$);  
* select a true correlation coefficient $\rho$;  
* generate ***X*** and ***Y*** to be bivariate normally distributed using a pseudo-Random Number Generator (RNG);
* using a pseudo-RNG, select the the number of bins for ***M*** and ***P*** ($t$ and $t'$) independantly from the set \{2, 3, 4, 5\};
* select the bin boundaries for ***M*** and ***P*** ($\boldsymbol{\theta}$ and $\boldsymbol{\theta}'$) by sorting the results of $(t-1)$ and $(t'-1)$ draws, respectively, from a normal distribution using a pseudo-RNG; 
* confirm that at least 2 levels of each of ***M*** and ***P*** are occupied (if not, retrun to generating ***X*** and ***Y***); and
* calculate and record relevant statistics.

When the exact method of selecting a parameter (such as $n$) is not noted in the above descriptions it is described as part of each simulation.

## Bias of the correlations

This sections shows the bias of the correlations as a function of the true correlation coefficient, $\rho$. To that end, a simulation was done at each level of the cartesian product of  $\rho \in \left( -0.99, -0.95, -0.90, -0.85, ..., 0.95, 0.99 \right)$, and $n \in \{10, 100, 1000\}$. For precision, each iteration is run fifty times. The bias is the mean difference between the true correlation coefficient ($\rho_i$) and estimate correlation coefficient ($r$). The RMSE is the square root of the mean squared error.

$$\mathrm{RMSE}= \sqrt{ \frac{1}{n} \sum_i \left( r_i - \rho_i \right)^2 }$$ 


This plot shows the bias as a function of the true correlation $\rho$. Only the Spearman correlation shows a clear trend with a positive bias below 0 (negative correlation) and a negative bias above 0 (positive correlation).

```{r bias vs rho, echo=FALSE,fig.width=6, fig.height=2.5}
bias$rmse <- sqrt( (bias$est - bias$rho)^2 )
bias$bias <- bias$est - bias$rho
agg <- summaryBy(bias + rmse  ~ n + rho + type, data=bias, FUN=mean, na.rm=TRUE)

xyplot(bias.mean ~ rho|type,
      data=agg,
      groups=n,
      type="l",
      ylab="Bias",
      xlab=expression(rho),
      scales=list(x=list(cex=0.7), y=list(cex=0.7)),
      auto.key=list(lines=TRUE, points=FALSE, space="right", cex=0.7),
      par.settings=list(superpose.line=list(lwd=2), plot.line=list(lwd=2)))
```

This plot shows the RMSE as a function of $\rho$. All of the correlation coefficients have a uniform RMSE as a function of $\rho$ near $\rho=0$ that decreases near $|\rho|=1$. All plots also show a decrease in RMSE as $n$ increases. That is further shown in the next plot.

```{r rmse vs rho, echo=FALSE,fig.width=6, fig.height=2.5}
xyplot(rmse.mean ~ rho|type,
      data=agg,
      groups=n,
      scales=list(y=list(log=10, cex=0.7), x=list(cex=0.7)),
      ylab="RMSE",
      xlab=expression(rho),
      type="l",
      auto.key=list(lines=TRUE, points=FALSE, space="right", cex=0.7),
      par.settings=list(superpose.line=list(lwd=2), plot.line=list(lwd=2)))
```

#### Consistency of the correlations  

This plot shows the RMSE as a function of $n$. The plot shows a slope of about $-\frac{1}{2}$, which suggests first order convergence. 

```{r rmse vs n, echo=FALSE,fig.width=6, fig.height=2.5}
agg <- summaryBy(rmse  ~ n+type, data=bias, FUN=mean, na.rm=TRUE)
xyplot(rmse.mean ~ n,
     groups=type,
      data=agg,
      ylab="RMSE",
      xlab="n",
      scales=list(y=list(log=10, cex=0.7), x=list(log=10, cex=0.7)),
      type="l",
      auto.key=list(lines=TRUE, points=FALSE, space="right", cex=0.7),
      par.settings=list(superpose.line=list(lwd=2), plot.line=list(lwd=2)))
```

#### Computing Time

Finally, this plot shows the mean time to compute a single correlation coefficient as a function of $\rho$ by $n$ size.

```{r time vs n, echo=FALSE,fig.width=6, fig.height=2.5}
agg <- summaryBy(t ~ n + type, data=ntime, FUN=mean, na.rm=TRUE)
agg$t.mean <- ifelse(agg$t.mean==0, 0.001,agg$t.mean)

xyplot(t.mean ~ n,
       data=agg,
       scales=list(y=list(log=10, cex=0.7), x=list(log=10, cex=0.7)),
       groups=type,
       type="l",
       ylab="Computing time",
       xlab="n",
       auto.key=list(lines=TRUE, points=FALSE, space="right", cex=0.7),
       par.settings=list(superpose.line=list(lwd=2), plot.line=list(lwd=2)))
```

## Simulation study of weighted correlations
In this section the consistency of the weighted coefficients is examined.

For the weighted case the unweighted result would be consistent if there were not something about the higher weight cases that makes them diffeent from the lower weight cases. Thus, while it is not reasonable to always assume that there is a difference between the high and low weight cases, both are possible and such circumstance serves as a more robust test of the methods in this package to consider a cases where they are associated. So, when complex sampling (other than simple random sampling with replacement) is used, unweighted correlations may or may not be consistent. The simulations below explore a case where the unweighted are not expected to be consistent to see if the weighted results are consistent. 

##Results of weighted correlation simulations

Simulations are carried out in the same fashion as previously described but include a few extra steps to accomodate weights. The following changes were made:

* to find the correlation between $n$ values, $5n$ values were generated.
* weights are assigned according to $w_i = (x-y)^2 + 1$, and the probability of inclusion in the sample was proportional to $Pr_i = \frac{\sum_j w_j}{w_i}$.
* a sample of size $n$ was then selected from the $5n$ values according to the proabilities; and weighted as well as unweighted correlation coefficients were calculated.

Two simulations were run. The first shows the mean absolute deviation (MAD) as a function of $\rho$ and was run for $n=100$ and  $\rho \in \left( -0.99, -0.95, -0.90, -0.85, ..., 0.95, 0.99 \right)$, with 100 iterations run for each value of $\rho$. 

The following plot shows the MAD for the weighted and unweighted results as a function of $\rho$ when $n=100$.

```{r wgt vs rho plot, echo=FALSE,fig.width=6, fig.height=2.5}
wgt <- wgtvrho
wgt$absdrho <- abs(wgt$est - wgt$rho)

agg <- summaryBy(absdrho ~ rho + usew + type, data=wgt, FUN=mean, na.rm=TRUE)
agg$weight <- ifelse(agg$usew, "Weighted", "Unweighted")

xyplot(absdrho.mean ~ rho|type,
       data=agg,
       groups=weight,
       scales=list(y=list(log=10, cex=0.7), x=list(cex=0.7)),
       type="l", 
       ylab="MAD",
       xlab=expression(rho),
       auto.key=list(lines=TRUE, points=FALSE, space="right", cex=0.7),
       par.settings=list(superpose.line=list(lwd=2), plot.line=list(lwd=2)))
```

The second simulation used the same values of $\rho$ and used $n \in \{10, 100, 1000, 10000, 100000 \}$ and shows how RMSE and sample size are related. In particular, it shows first order convergence of the weighted correlation coefficient. Again, the fact that the simulations show that the unweighted correlation coefficient is not consistent is not meant to imply that it will always be that way--only that this is possible.

```{r wgt v n plot, echo=FALSE,fig.width=6, fig.height=2.5}
wgt <- wgtvn
wgt$mserho <- (wgt$est - wgt$rho)^2

agg <- summaryBy(mserho ~ n + usew + type, data=wgt, FUN=mean, na.rm=TRUE)
agg$rmserho <- sqrt(agg$mserho)
agg$weight <- ifelse(agg$usew, "Weighted", "Unweighted")

xyplot(rmserho ~ n|type,
       data=agg,
       groups=weight,
       scales=list(y=list(log=10, cex=0.7), x=list(log=10, cex=0.7)),
       type="l",
       ylab="RMSE",
       xlab="n",
       auto.key=list(lines=TRUE, points=FALSE, space="right", cex=0.7),
       par.settings=list(superpose.line=list(lwd=2), plot.line=list(lwd=2)))
```



