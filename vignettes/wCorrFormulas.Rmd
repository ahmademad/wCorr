---
title: "wCorr Formulas"
author: "Paul Bailey, Ahmad Emad, Ting Zhang, Qingshu Xie"
date: '`r Sys.Date()`'
output: pdf_document
vignette: >
  %\VignetteIndexEntry{wCorr Formulas}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r packages and data, echo=FALSE, results="hide", message=FALSE, warning=FALSE}
require(wCorr)
require(lattice)
require(doBy)
require(captioner)
# set layout so a figure label appears to go with the figure
trellis.device()
trellis.par.set(list(layout.widths  = list(left.padding = 3, right.padding = 3),
                     layout.heights = list(top.padding = -1, bottom.padding = 3)))
load("..//vignettes/sim/ntime.RData")
load("..//vignettes/sim/bias.RData")
load("..//vignettes/sim/wgtvrho.RData")
load("..//vignettes/sim/wgtvn.RData")
```


```{r tables and figures, echo=FALSE, results="hide", message=FALSE,warning=FALSE}
fig_nums <- captioner()
table_nums <- captioner(prefix = "Table")

theta <- fig_nums("theta", "")
biasVsRho <- table_nums("biasVsRho", "")
rmseVsRho <- table_nums("rmseVsRho", "")
rmseVsRho2 <- table_nums("rmseVsN", "")
speedi <- table_nums("speedi", "")
rmseVsRho3 <- table_nums("rmseVsRho2", "")
rmseVsN <- table_nums("rmseVsN2", "")
```

The wCorr package can be used to calculate Pearson, Spearman, polyserial, and polychoric correlations, in weighted or unweighted form.^[The estimation procedure used by the wCorr package for the polyserial is based on the likelihood function in by Cox, N. R. (1974), "Estimation of the Correlation between a Continuous and a Discrete Variable." *Biometrics*, **30** (1), pp 171-178. The likelihood function for polychoric is from Olsson, U. (1979) "Maximum Likelihood Estimation of the Polychoric Correlation Coefficient." *Psyhometrika*, **44** (4), pp 443-460. The likelihood used for Pearson and Spearman is written down many places. One is the "correlate" function in Stata Corp, Stata Statistical Software: Release 8. College Station, TX: Stata Corp LP, 2003.] The package implements the tetrachoric correlation as a specific case of the polychoric correlation and biserial correlation as a specific case of the polyserial correlation. When weights are used, the correlation coefficients are calculated with so called sample weights or inverse probability weights.^[Sample weights are comparable to `pweight` in Stata.]
 
This vignette introduces the methodology used in the wCorr package for computing the Pearson, Spearman, polyserial, and polychoric correlations, with and without weights applied. For the polyserial and polychoric correlations, the coefficient is estimated using a numerical likelihood maximization. The weighted (and unweighted) likelihood functions that are used are described.

We present formulas that are used to compute correlations and then present simulation evidence to showcorrectness of the methods, including an examination of the bias and consistency of both methods. This is done seperately for unweighted and weighted correlations. For this exercise, numerical simulations are used to show:

* the bias of the methods as a function of the true correlation coefficient ($\rho$) and the number of observations ($n$) in the unweighted and weighted cases; and
* the accuracy [measured with root mean squared error (RMSE) and mean absolute deviation (MAD)] of the methods as a function of $\rho$ and $n$ in the unweighted and weighed cases.

Note that here "bias" is used to mean the mean difference between true correlation and estimated correlation.

The *wCorr Arguments* vignette describes the effects the `ML` and `fast` arguments have on computation and gives examples of calls to wCorr.

# Specification of estimation formulas 
Here we focus on specification of the correlation coefficients between two vectors of random variables that are jointly bivariate normal. We call the two vectors ***X*** and ***Y***. The $i^{th}$ members of the vectors are then called $x_i$ and $y_i$.

## Computation for Pearson and Spearman correlations 

The weighted Pearson correlation is computed using the formula
$$\rho_{Pearson}=\frac{\sum_{i=1}^n \left[ w_i (x_i-\bar{x})(y_i-\bar{y}) \right]}{\sqrt{\sum_{i=1}^n \left( w_i  (x-\bar{x})^2 \right)\sum_{i=1}^n \left( w_i  (y-\bar{y})^2 \right) }} $$

where $w_i$ is the weights, $\bar{x}$ is the weighted mean of the ***X*** variable ($\bar{x}=\frac{1}{\sum_{i=1}^n w_i}\sum_{i=1}^n w_i x_i$), $\bar{y}$ is the weighted mean of the ***Y*** variable ($\bar{y}=\frac{1}{\sum_{i=1}^n w_i}\sum_{i=1}^n w_i y_i$), and $n$ is the number of elements in ***X*** and ***Y***.^[See the "correlate" function in Stata Corp, Stata Statistical Software: Release 8. College Station, TX: Stata Corp LP, 2003.]

The unweighted Pearson correlation is calculated by setting all of the weights to one.

For the Spearman correlation coefficient the unweighted coefficient is calculated by ranking the data and then using those ranks to calculate the Pearson correlation coefficient--so the ranks stand in for the $x$ and $y$ data. Again, similar to the Pearson, for the unweighted case the weights are all set to one.

There is no commonly accepted weighted Spearman correlation coefficient. Nevertheless, we implement one possible weighted Spearman correlation coefficient as follows. First, the rank scores of the data are calculated (this ranking is unweighted) and then the ranks are passed to the Pearson correlation coefficient formula shown above, and this Pearson correlation coefficient does use the weights.  

For both cases the same ranking formula is used so that the highest value recieves a value of 1 and the second highest 2, and so on down to the $n$th value. In addition, when data are ranked, ties must be handled in some way. The chosen method is to assign the average of all tied ranks. For example, if the second and third rank units are tied then both units would receive a rank of 2.5 (the average of 2 and 3).

##Methodology for polyserial correlation with and without weights
For the polyserial correlation, it is again assumed that there are two continuous variables ***X*** and ***Y*** that have a bivariate normal distribution.^[For a more complete treatment of the polyserial correlation, see Cox, N. R., "Estimation of the Correlation between a Continuous and a Discrete Variable" *Biometrics*, **50** (March), 171-187, 1974.]

$$\begin{pmatrix} X \\ Y \end{pmatrix} \sim N \left[ \begin{pmatrix} \mu_x \\ \mu_y \end{pmatrix}, \Sigma \right] $$

Where $N(A, \Sigma)$ is a bivariate normal distribution with mean vector $A$ and covariance matrix $\Sigma$. For the polyserial correlation, ***Y*** is discretized into the random variable ***M*** according to

$$m_i= \begin{cases} 1 \quad \mathrm{if}  \theta_2 < y_i < \theta_3 \\ 2 \quad \mathrm{if} \theta_3 < y_i < \theta_4 \\ \vdots \\ t \quad \mathrm{if} \theta_{t+1} < y_i < \theta_{t+2} \end{cases}$$

where, $\theta$ are the cut points used to discretize ***Y*** into ***M***, and $t$ is the number of bins. For notational convenience, $\theta_2 \equiv -\infty$ and $\theta_{t+2} \equiv \infty$.^[The indexing is somewhat odd to be consistent with Cox (1974). Nevertheless, this treatment does not use the Cox definition of $\theta_0$, $\theta_1$ or $\theta_2$ which are either not estimated (as is the case for $\theta_0$, and $\theta_1$) or are reappopriated (as is the case for $\theta_2$). Cox calls the correlation coefficient $\theta_2$ while this document uses $\rho$ and uses $\theta_2$ to store $-\infty$ as a convenience so that the vector $\boldsymbol{\theta}$ includes the (infinite) bounds as well as the interior points.]

To give a concrete example, the following figure shows the density of ***Y*** when the cuts points are, for this example, $\theta=\left(-\infty,-2,-0.5,1.6,\infty\right)$. In this example, any value of $-2 < y_i < -0.5$ would have $m_i=2$.

\newpage
  
**`r fig_nums("theta", display="cite")`.** *Density of Y for cutpoints $\theta \in (-\infty, -2, -0.5, 1.6, \infty$).*    
```{r theta,echo=FALSE,results="hide",fig.width=7, fig.height=5.5}
x <- seq(-3,3,by=0.01) 
y <- dnorm(x)
par0 <- par(no.readonly=TRUE)
par(ann=FALSE)
par(mar=c(5,2,1,1)+0.1)
plot(x,y,type="l",xlab="y",ylab="Density", xaxt="n", yaxt="n")
axis(1,at=c(-2,-0.5,1.6), labels=expression(theta[3],theta[4],theta[5]))
text(x=c(-2.5,-1.25,0.55,2.3),y=0.05, labels=paste0("m=",1:4))
theta <- c(-2,-0.5,1.6)
for(i in 1:3) {
  lines(rep(theta[i],2), c(-1,dnorm(theta[i])))
}
par(ann=TRUE)
par(mgp=c(0.5,0,0))
title(ylab="density")
par(mgp=c(3,1,0))
title(xlab="Y")
par(par0)
```

Notice that $\mu_y$ is not identifed (or is irrelevant) because, for any $a \in {\rm I\!R}$, setting $\tilde{\mu}_y = \mu_y + a$ and $\tilde{\boldsymbol{\theta}}=\boldsymbol{\theta} + a$ lead to exactly the same values of $\mathbf{M}$ and so one of the two must be arbitrarily assigned. A convenient decision is to decide $\mu_y \equiv 0$. A similar argument holds for $\sigma_y$ so that  $\sigma_y \equiv 1$.

For ***X***, Cox (1974) observes that the MLE mean and standard deviation of ***X*** are simply the average and (population) standard deviation of the data and do not depend on the other parameters.^[The population standard devaition is used because it is the MLE for the standard deviation. Notice that, while the sample variance is an unbiased estimator of the variance and the population variance is not an unbaised estimator of the variance, they are very similar and the variance is also a nuisance parameter, not a parameter of interest when finding the correlation.] This can be taken advantage of by defining $z$ to be the standardized score of $x$ so that $z \equiv \frac{x- \bar{x}}{ \hat\sigma_x}$. 

Combining these simplifications, the probability of any given $x_i$, $m_i$ pair is

$$\mathrm{Pr}\left( \rho=r, \theta ; Z=z_i, M=m_i \right) = \phi(z_i) \int_{\theta_{m_i+1}}^{\theta_{m_i+2}} \mkern-40mu  f(y|Z=z,\rho=r)dy$$

where $\mathrm{Pr}\left( \rho=r, \theta; Z=z_i, M=m_i \right)$ is the probability of the event $\rho=r$ and the cuts points are $\theta$, given the data $z_i$ and $m_$; $\phi(\cdot)$ is the standard normal; and $f(y|z,\rho)$ is the distribution of $y$ conditional on $z$ and $\rho$. Because $y$ and $z$ are jointly normally distributed (by assumption)
$$f(y|Z=z,\rho=r) =N\left(\mu_y + \frac{\sigma_y}{\sigma_z}r(z-\mu_z), (1-r^2){\sigma_y}^2 \right)$$
because both ***Z*** and ***Y*** are standard normals
$$f(y|Z=z,\rho=r) =N\left(r \cdot z, (1-r^2) \right)$$
now, define $w \equiv \frac{y-r\cdot z}{\sqrt{1-r^2}}$ and $w$ has a standard normal distribution. Plugging this in
$$\mathrm{Pr}\left( \rho=r, \theta ; Z=z_i, M=m_i \right) = \phi(z_i) \left[ \Phi\left( \frac{\theta_{m_i+2} - r \cdot z_i}{\sqrt{1-r^2}} \right) - \Phi \left( \frac{\theta_{m_i+1} - r \cdot z_i}{\sqrt{1-r^2}} \right) \right]$$

Where $\Phi(\cdot)$ is the standard normal cumulative density function. Using the above probability function as an objective, the log-likelihood is then maximized.
$$\ell(\rho, \theta;z,m) = \sum_{i=1}^n w_i \ln\left[ \mathrm{Pr}\left( \rho=r, \theta ; Z=z_i, M=m_i \right) \right]$$
where $w_i$ is the weight of the $i^{th}$ members of the vectors ***Z*** and ***Y***. For the unweighted case, all of the weights are set to one.

The value of the nuisance paramter $\boldsymbol{\theta}$ is chosen to be $\Phi^{-1}(n/N)$ where $n$ is the number of values to the left of the cut point ($\theta_i$ value) and $N$ is the number of data points overall. For the weighted cause $n$ is replaced by the sum of the weights to the left of the cut point and $N$ is replaced by the total weight of all units.

###Computation of polyserial correlation
The derivatives of $\ell$ can be written down but are not readily computed. When the `ML` argumet is set to `FALSE` (the default), a one dimensional optimization of $\rho$ is calculated using `stats::optimize` at the values of $\boldsymbol{\theta}$ from the previous paragraph. When the `ML` argument is set to `TRUE`, a multi-dimensional optimization is done for $\rho$ and $\boldsymbol{\theta}$ using `minqa::bobyqa`. See the *wCorr Arguments* vignette for a comparison of these two methods.

Because the numerical optimization is not perfect when the correlation is in a boundary condition ($\rho \in \{-1,1\}$), a check for perfect correlation  is performed before the above optimization by simply examining if the values of ***X*** and ***M*** have agreeing order (or opposite but agreeing order) and then the MLE correlation of 1 (or -1) is returned.

##Methodology for polychoric correlation with and without weights

Similar to the polyserial correlation, the polychoric correlation is a simple case of two continuous variables ***X*** and ***Y*** that have a bivariate normal distribution. In the case of the polyserial correlation the continuous (latent) variable ***Y*** was observed as a discretized variable ***M***. For the polychoric correlation, this is again true but now the continuous (latent) variable ***X*** is observed as a discrete variable ***P*** according to

$$p_i= \begin{cases} 1 \quad \mathrm{if}  \theta'_2 < x_i < \theta'_3 \\ 2 \quad \mathrm{if} \theta'_3 < x_i < \theta'_4 \\ \vdots \\ t \quad \mathrm{if} \theta'_{t+1} < x_i < \theta'_{t+2} \end{cases}$$

where $\boldsymbol{\theta}$ remains the cut points for the distibution defining the transformation of ***Y*** to ***M*** and $\boldsymbol{\theta}'$ is the cut points for the transformation from ***X*** to ***P***. Similar to $\boldsymbol{\theta}$, $\boldsymbol{\theta}'$ has $\theta_2' \equiv -\infty$ and $\theta_{t+2}' \equiv \infty$.

As in the polyserial correlation, $\mu_y$ is not identifed (or is irrelevant) because, for any $a \in {\rm I\!R}$, setting $\tilde{\mu}_y = \mu_y + a$ and $\tilde{\boldsymbol{\theta}}'=\boldsymbol{\theta}' + a$ lead to exactly the same values of $\mathbf{M}$ and so one of the two must be arbitrarily assigned. The same is true for $\mu_x$. A convenient decision is to decide $\mu_y = \mu_x \equiv 0$. A similar argument holds for $\sigma_y$ and $\sigma_x$ so that  $\sigma_y = \sigma_x \equiv 1$

Then the probability of any given $m_i$, $p_i$ pair is 

$$\mathrm{Pr}\left( \rho=r, \theta, \theta' ; P=p_i, M=m_i \right) = \int_{\theta_{p_i+1}'}^{\theta_{p_i+2}}  \int_{\theta_{m_i+1}}^{\theta_{m_i+2}} \mkern-40mu  f(x,y|\rho=r)dydx$$

where $\rho$ is the correlation coefficient.

Using this function as an objective, the log-likelihood is then maximized.
$$\ell(\rho, \theta, \theta';p,m) = \sum_{i=1}^n w_i \ln\left[\mathrm{Pr}\left( \rho=r, \theta, \theta' ; P=p_i, M=m_i \right) \right] $$

This is the weighted log-likelihood function. For the unweighted cause all of the weights are set to one.

###Computation of polychoric correlation
This again mirrors the treatment of the polyserial. The derivatives of $\ell$ can be written down but are not readily computed. When the `ML` argument is set to `FALSE` (the default), a one dimensional optimization of $\rho$ is calculated using `stats::optimize`. When the `ML` argument is set to `TRUE` a multi-dimensional optimization is done for $\rho$, $\boldsymbol{\theta}$, and $\boldsymbol{\theta}'$ using `minqa::bobyqa`. See the *wCorr Arguments* vignette for a comparison of these two methods.

Because the optimization is not perfect when the correlation is in a boundary condition ($\rho \in \{-1,1\}$), a check for perfect correlation  is performed before the above optimization by simply examining if the values of ***P*** and ***M*** have a Goodman-Kruskal correlation coefficient of -1 or 1. When this is the case, the MLE of -1 or 1, respectively, is returned.

# Correctness of the estimating methods

It is easy to prove the consistency of the $\boldsymbol{\theta}$ for the polyserial correlation and $\boldsymbol{\theta}$ and $\boldsymbol{\theta}'$ for the polychoric correlation using the non-ML case. Similarly, for $\rho$, because it is an MLE that can be obtained by taking a derivative and setting it equal to zero, the results are asymtotically unbiased and obtain the Cramer-Rao lower bound.

This does not speak to the small sample properties of these correlation coefficients. Previous work has described their properties by simulation; and that tradition is continued below.

## Simulation study of unweighted correlations

In what follows, when the exact method of selecting a parameter (such as $n$) is not noted in the above descriptions it is described as part of each simulation.

Across a number of iterations (the exact number of times will be stated for each simulation), the following procedure is used:

* select a true correlation coefficient $\rho$;
* select the number of observations ($n$);
* generate ***X*** and ***Y*** to be bivariate normally distributed using a pseudo-Random Number Generator (RNG);
* using a pseudo-RNG, select the the number of bins for ***M*** and ***P*** ($t$ and $t'$) independantly from the set \{2, 3, 4, 5\};
* select the bin boundaries for ***M*** and ***P*** ($\boldsymbol{\theta}$ and $\boldsymbol{\theta}'$) by sorting the results of $(t-1)$ and $(t'-1)$ draws, respectively, from a normal distribution using a pseudo-RNG; 
* confirm that at least 2 levels of each of ***M*** and ***P*** are occupied (if not, retrun to generating ***X*** and ***Y***); and
* calculate and record relevant statistics.



## Bias, and RMSE of the unweighted correlations

This sections shows the bias of the correlations as a function of the true correlation coefficient, $\rho$. To that end, a simulation was done at each level of the cartesian product of  $\rho \in \left( -0.99, -0.95, -0.90, -0.85, ..., 0.95, 0.99 \right)$, and $n \in \{10, 100, 1000\}$. For precision, each level of $\rho$ and $n$ was run fifty times. The bias is the mean difference between the true correlation coefficient ($\rho_i$) and estimate correlation coefficient ($r$). The RMSE is the square root of the mean squared error.

$$\mathrm{RMSE}= \sqrt{ \frac{1}{n} \sum_{i=1}^n \left( r_i - \rho_i \right)^2 }$$ 

And the bias is given by

$$bias=\frac{1}{n}\sum_{i=1}^n \left(r_i - \rho_i \right)$$

This plot shows the bias as a function of the true correlation $\rho$. Only the Spearman correlation shows a clear trend with a positive bias below 0 (negative correlation) and a negative bias above 0 (positive correlation). The other estimators show no clear bias.

**`r fig_nums("biasVsRho", display="cite")`.** *Mean Absolute Deviance vs $\rho$ (Unweighted).*    
```{r bias vs rho, echo=FALSE,fig.width=7, fig.height=5}
bias$rmse <- sqrt( (bias$est - bias$rho)^2 )
bias$bias <- bias$est - bias$rho
agg <- summaryBy(bias + rmse  ~ n + rho + type, data=bias, FUN=mean, na.rm=TRUE)

xyplot(bias.mean ~ rho|type,
      data=agg,
      groups=n,
      type="l",
      ylab="Bias",
      xlab=expression(rho),
      scales=list(x=list(cex=0.7), y=list(cex=0.7)),
      auto.key=list(lines=TRUE, points=FALSE, space="right", cex=0.7),
      par.settings=list(superpose.line=list(lwd=2), plot.line=list(lwd=2)))
```

This plot shows the RMSE as a function of $\rho$. All of the correlation coefficients have a uniform RMSE as a function of $\rho$ near $\rho=0$ that decreases near $|\rho|=1$. All plots also show a decrease in RMSE as $n$ increases. This plot shows that there is no appreciable RMSE differences as a functions of $\rho$. In addition, it show that our attention to the MLE correlation of -1 or 1 at edge cases did not make the RMSE much worse in the neighborhood of the edges ($|\rho| \sim 1$).

**`r fig_nums("rmseVsRho", display="cite")`.** *Root Mean Square Error vs $\rho$ (Unweighted).*   
```{r rmse vs rho, echo=FALSE,fig.width=7, fig.height=5.5}
xyplot(rmse.mean ~ rho|type,
      data=agg,
      groups=n,
      scales=list(y=list(log=10, cex=0.7), x=list(cex=0.7)),
      ylab="RMSE",
      xlab=expression(rho),
      type="l",
      auto.key=list(lines=TRUE, points=FALSE, space="right", cex=0.7),
      par.settings=list(superpose.line=list(lwd=2), plot.line=list(lwd=2)))
```

#### Consistency of the correlations  

This plot shows the RMSE as a function of $n$. The plot shows a slope of about $-\frac{1}{2}$, which is consistent with the expected first order convergence for each correlation coefficient under the assumptions of this simulation. Thus, the RMSE is  small for n > 100. 

\newpage 

**`r fig_nums("rmseVsN", display="cite")`.** *Root Mean Square Error vs sample size (Unweighted).*   
```{r rmse vs n, echo=FALSE,fig.width=7, fig.height=5.5}
agg <- summaryBy(rmse  ~ n+type, data=bias, FUN=mean, na.rm=TRUE)
xyplot(rmse.mean ~ n,
     groups=type,
      data=agg,
      ylab="RMSE",
      xlab="n",
      scales=list(y=list(log=10, cex=0.7), x=list(log=10, cex=0.7)),
      type="l",
      auto.key=list(lines=TRUE, points=FALSE, space="right", cex=0.7),
      par.settings=list(superpose.line=list(lwd=2), plot.line=list(lwd=2)))
```

#### Computing Time

This plot shows the mean time (in seconds) to compute a single correlation coefficient as a function of $\rho$ by $n$ size. The plot shows linearly rising computation times with slopes of about one. This is consistent with a linear computation cost. Using Big O notation, the computation cost is, in the range shown, O($n$).

\newpage
  
**`r fig_nums("speedi", display="cite")`.** *Computation time.*    
```{r time vs n, echo=FALSE,fig.width=7, fig.height=5.5}
agg <- summaryBy(t ~ n + type, data=ntime, FUN=mean, na.rm=TRUE)
agg$t.mean <- ifelse(agg$t.mean==0, 0.001,agg$t.mean)

xyplot(t.mean ~ n,
       data=agg,
       scales=list(y=list(log=10, cex=0.7), x=list(log=10, cex=0.7)),
       groups=type,
       type="l",
       ylab="Computing time (s)",
       xlab="n",
       auto.key=list(lines=TRUE, points=FALSE, space="right", cex=0.7),
       par.settings=list(superpose.line=list(lwd=2), plot.line=list(lwd=2)))
```

## Simulation study of weighted correlations
When complex sampling (other than simple random sampling with replacement) is used, unweighted correlations may or may not be consistent. In this section the consistency of the weighted coefficients is examined.

When generating simulated data, decisions about the generating functions have to be made. These decisions affect how the results are interpreted. For the weighted case, if these decisions lead to something about the higher weight cases being different from the lower weight cases then the test will be more informative about the role of weights. Thus, while it is not reasonable to always assume that there is a difference between the high and low weight cases, the assumption (used in the simulations below) that there is an association between weight and the correlaiton serves as a more robust test of the methods in this package.

##Results of weighted correlation simulations

Simulations are carried out in the same fashion as previously described but include a few extra steps to accomodate weights. The following changes were made:

* weights are assigned according to $w_i = (x-y)^2 + 1$, and the probability of inclusion in the sample was then ${\rm Pr}_i = \frac{1}{w_i}$.
* For each unit, a uniformily distributed random number was drawn. When that value was less than the probability of inclusion (${\rm Pr}_i$), the unit was included.

Units were generated until $n$ units were in the sample.

Two simulations were run. The first shows the mean absolute deviation (MAD)

$$MAD=\frac{1}{n}\sum_{i=1}^n |r_i - \rho_i|$$

as a function of $\rho$ and was run for $n=100$ and  $\rho \in \left( -0.99, -0.95, -0.90, -0.85, ..., 0.95, 0.99 \right)$, with 100 iterations run for each value of $\rho$. 

The following plot shows the MAD for the weighted and unweighted results as a function of $\rho$ when $n=100$. This shows that for values of $\rho$ near zero, under our simulation assumptions, the weighted correlation performs better than (has lower MAD than) the unweighted correlation for all correlation coefficients. Over the entire range, the difference between the two is never such that the unweighted has a lower MAD.

**`r fig_nums("rmseVsRho2", display="cite")`.** *Root Mean Square Error vs $\rho$ (Weighted).*   
```{r wgt vs rho plot, echo=FALSE,fig.width=7, fig.height=5.5}
wgt <- wgtvrho
wgt$absdrho <- abs(wgt$est - wgt$rho)

agg <- summaryBy(absdrho ~ rho + usew + type, data=wgt, FUN=mean, na.rm=TRUE)
agg$weight <- ifelse(agg$usew, "Weighted", "Unweighted")

xyplot(absdrho.mean ~ rho|type,
       data=agg,
       groups=weight,
       scales=list(y=list(log=10, cex=0.7), x=list(cex=0.7)),
       type="l", 
       ylab="MAD",
       xlab=expression(rho),
       auto.key=list(lines=TRUE, points=FALSE, space="right", cex=0.7),
       par.settings=list(superpose.line=list(lwd=2), plot.line=list(lwd=2)))
```


The second simulation used the same values of $\rho$ and used $n \in \{10, 100, 1000, 10000, 100000 \}$ and shows how RMSE and sample size are related. In particular, it shows first order convergence of the weighted Pearson and polyserial correlation coefficient. The polychoric and Spearman correlations are not consistent and have minimum RMSE values indicating that some residual error always exists--under the assumptions of our simulation. In all cases the RMSE is lower for the weighted than the unweighted. Again, the fact that the simulations show that the unweighted correlation coefficient (and some of the weighted correlation coefficients) is not consistent does not imply that it will always be that way--only that this is possible for these coefficients to not be consistent.

**`r fig_nums("rmseVsN2", display="cite")`.** *Root Mean Square Error vs $\rho$ (Weighted).*   
```{r wgt v n plot, echo=FALSE,fig.width=7, fig.height=5.5}
wgt <- wgtvn
wgt$mserho <- (wgt$est - wgt$rho)^2

agg <- summaryBy(mserho ~ n + usew + type, data=wgt, FUN=mean, na.rm=TRUE)
agg$rmserho <- sqrt(agg$mserho)
agg$weight <- ifelse(agg$usew, "Weighted", "Unweighted")

xyplot(rmserho ~ n|type,
       data=agg,
       groups=weight,
       scales=list(y=list(log=10, cex=0.7), x=list(log=10, cex=0.7)),
       type="l",
       ylab="RMSE",
       xlab="n",
       auto.key=list(lines=TRUE, points=FALSE, space="right", cex=0.7),
       par.settings=list(superpose.line=list(lwd=2), plot.line=list(lwd=2)))
```

#Conclusion
Overall the simulations show first order convergence for each unweighted correlation coefficient with a linear computation cost. Further, under our simulation assumptions, the weighted correlation performs better than (has lower MAD than) the unweighted correlation for all correlation coefficients.

We show the first order convergence of the weighted Pearson and polyserial correlation coefficient. The polychoric and Spearman are not consistent and have minimum RMSE values indicating that some residual error always exists--under the assumptions of our simulation. In all cases the RMSE is lower for the weighted than the unweighted.