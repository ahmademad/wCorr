---
title: "wCorr Formulas"
author: "Paul Bailey, Ahmad Emad, (people who do QC for this product)"
date: '`r Sys.Date()`'
output: pdf_document
vignette: |
  %\VignetteIndexEntry{wCorr Formulas} %\VignetteEngine{knitr::rmarkdown} %\VignetteEncoding{UTF-8}
---

```{r packages and data, echo=FALSE, results="hide", message=FALSE}
require(wCorr)
require(lattice)
require(doBy)
load("Q:/Paul/wCorr/wCorr/vignettes/ntime.RData")
load("Q:/Paul/wCorr/wCorr/vignettes/bias.RData")
```

This document shows the methodology used in computing the correlations in wCorr for the weighted Pearson, Spearman, polyserial, and polychoric correlations. For the polyserial and polychoric correlations the coefficient is estimated using a numerical likelihood maximization. The weighted likelihood functions that are used are motivated and described.

Additionally, for the polyserial and polychoric evidence is offered for the correctness of the methods, including a sketch of a proof of the consistency of both methods and numerical simulations to show:
* The bias of the methods as a function of the true correlation coefficient ($\rho$) and the number of observations ($n$)
* The root mean square error (RMSE) of the methods as a function of $\rho$ and $n$

# Methodology
Here we focus on measurement of the correlation coefficients between two vectors of random variables that are jointly bivariate normal--call the vectors ***X*** and ***Y***.^[The Spearman correlation coefficient can be motivated by a much more broad class of random variables but that is not dealt with in this document.] The $i^{th}$ members of the vectors are then called $x_i$ and $y_i$.

##Pearson and Spearman methodology

The Pearson correlation is computed using the formula
$$\rho_{Pearson}=\frac{\sum_i \left[ w_i (x_i-\bar{x})\times(y_i-\bar{y}) \right]}{\sqrt{\sum_i \left( w_i \times (x-\bar{x})^2 \right) + \sum_i \left( w_i \times (y-\bar{y})^2 \right) }} $$

where $\bar{x}=\frac{1}{\sum w_i}\sum_i w_i x_i$, $\bar{y}=\frac{1}{\sum w_i}\sum_i w_i y_i$, and $n$ is the number of elements in ***X*** and ***Y***.^[See the "correlate" function in Stata Corp, Stata Statistical Software: Release 8. College Station, TX: Stata Corp LP, 2003.]

The Spearman correlation coefficient is calculated by first taking the rank of the data before the same formula is applied. When data are ranked ties must be handled in some way. The chosen method is to assign the average of all tied ranks. For example, if the second and third rank units are tied then both units would receive a rank of 2.5 (the average of 2 and 3).

##Polyserial methodology
For the polyserial correlation, it is again imagined that there are two continuous variables ***X*** and ***Y*** that have a bivariate normal distribution.^[For a more complete treatment of the polyserial correlation, see Cox, N. R., "Estimation of the Correlation between a Continuous and a Discrete Variable" *Biometrics*, **50** (March), 171-187, 1974.]

$$\begin{pmatrix} X \\ Y \end{pmatrix} \sim N \left[ \begin{pmatrix} \mu_x \\ \mu_y \end{pmatrix}, \Sigma \right] $$

Where $N(a, \Sigma)$ is a bivariate normal distribution with mean $a$ and variance $\Sigma$. For the polyserial, ***Y*** is discretized into the random variable ***M*** according to

$$m_i= \begin{cases} 1 \quad \mathrm{if}  \theta_2 < y_i < \theta_3 \\ 2 \quad \mathrm{if} \theta_3 < y_i < \theta_4 \\ \vdots \\ t \quad \mathrm{if} \theta_{t+1} < y_i < \theta_{t+2} \end{cases}$$

where, for notational convenience, $\theta_2 \equiv -\infty$ and $\theta_{t+2} \equiv \infty$.^[The indexing is somewhat odd to be consistent with Cox (1974). Nevertheless, this treatment does not use the Cox definition of $\theta_0$, $\theta_1$ or $\theta_2$ which are either not estimated (as is the case for $\theta_0$, and $\theta_1$) or are reappopriated (as is the case for $\theta_2$). Cox calls the correlation coefficient $\theta_2$ while this document uses $\rho$ and uses $\theta_2$ to store $-\infty$ as a convenience so that the vector $\boldsymbol{\theta}$ includes the (infinite) bounds as well as the interior points.]

The following figure shows the density of ***Y*** when $\theta=\left(-\infty,-2,-0.5,1.6,\infty\right)$. Here, for example, any value of $-2 < y_i < -0.5$ would have $m_i=2$.

```{r theta,echo=FALSE,results="hide",fig.show="hold",fig.width=6, fig.height=2.5}
x <- seq(-3,3,by=0.01) 
y <- dnorm(x)
par0 <- par(no.readonly=TRUE)
par(ann=FALSE)
par(mar=c(5,2,1,1)+0.1)
plot(x,y,type="l",xlab="y",ylab="density", xaxt="n", yaxt="n")
axis(1,at=c(-2,-0.5,1.6), labels=expression(theta[3],theta[4],theta[5]))
text(x=c(-2.5,-1.25,0.55,2.3),y=0.05, labels=paste0("m=",1:4))
theta <- c(-2,-0.5,1.6)
for(i in 1:3) {
  lines(rep(theta[i],2), c(-1,dnorm(theta[i])))
}
par(ann=TRUE)
par(mgp=c(0.5,0,0))
title(ylab="density")
par(mgp=c(3,1,0))
title(xlab="Y")
par(par0)
```

Notice that $\mu_y$ is not identifed (or is irrelevant) because setting $\tilde{\mu}_y = \mu_y + a$ and $\tilde{\boldsymbol{\theta}}=\boldsymbol{\theta} + a$ lead to exactly the same values of $\mathbf{M}$ and so one of the two must be artibrary assigned. A convenient decision is to decide $\mu_y \equiv 0$. A similar argument holds for $\sigma_y$ so that  $\sigma_y \equiv 1$.

For ***X*** Cox (1974) observes that the MLE mean and standard deviation of ***X*** are simply the average and (population) standard deviation of the data and do not depend on the other parameters.^[The population standard devaition is used because it is the MLE for the standard deviation. Notice that, while the sample variance is an unbiased estimator of the variance and the population variance is not an unbaised estimator of the variance, they are very similar and the variance is also a nusiance parameter, not a parameter of interest when finding the correlation.] This can be taken advantage of by defining $z \equiv \frac{x- \bar{x}}{ \hat\sigma_x}$. 

Combining these simplifications the probability of any given $x_i$, $m_i$ pair is

$$\mathrm{Pr}\left( \rho=r ; Z=z_i, M=m_i \right) = \phi(z_i) \int_{\theta_{m_i+1}}^{\theta_{m_i+2}} \mkern-40mu dy f(y|Z=z,\rho=r)$$

where $\mathrm{Pr}\left( \rho=r ; Z=z, M=m \right)$ is the probability of the event $\rho=r$ given the data $z$ and $m$, $\phi(\cdot)$ is the standard normal and $f(y|z,\rho)$ is the distribution of $y$ conditional on $z$ and $rho$. Because $y$ and $z$ are jointly bivariate normally distributed (by assumption)
$$f(y|Z=z,\rho=r) =N\left(\mu_y + \frac{\sigma_y}{\sigma_z}r(z-\mu_z), (1-r^2)\sigma_y \right)$$
because both ***Z*** and ***Y*** are standard normals
$$f(y|Z=z,\rho=r) =N\left(r \cdot z, (1-r^2) \right)$$
now, define $w \equiv \frac{y-r\cdot z}{1-r^2}$ and $w$ has a standard normal distribution. Plugging this in
$$\mathrm{Pr}\left( \rho=r, \theta ; Z=z_i, M=m_i \right) = \phi(z_i) \left[ \Phi\left( \frac{\theta_{m_i+2} - r \cdot z_i}{\sqrt{1-r^2}} \right) - \Phi \left( \frac{\theta_{m_i+1} - r \cdot z_i}{\sqrt{1-r^2}} \right) \right]$$

Where $\Phi(\cdot)$ is the standard normal cumulative density function. Using the above probability function as an objective, the log-likelihood is then maximized.
$$\ell(\rho;z,m) = \sum_i w_i \ln\left[ \mathrm{Pr}\left( \rho=r ; Z=z_i, M=m_i \right) \right]$$
where $w_i$ is the weight of the $i^{th}$ case.

The value of the nusiance paramter $\boldsymbol{\theta}$ is chosen to be $\Phi^{-1}(n/N)$ where $n$ is the number of values to the left of the cut point ($\theta_i$ value) and $N$ is the number of data points overall.

###Polyserial computation
The derivatives of $\ell$ can be computed but are not readily computed and so when the `ML` argumet is set to `FALSE` (the default) a one dimensional optimization of $\rho$ is calculated using `stats::optimize`. When the `ML` argument is set to `TRUE` a multi-dimensional optimization is done for $\rho$ and $\boldsymbol{\theta}$ using `minqa::bobyqa`. As is shown below, the difference between these two is slight, if present, and so the default value of `ML` is recomended.

Because the optimization is not perfect when the correlation is in a boundary condition ($\rho \in \{-1,1\}$), a check for perfect correlation  is performed before the above optimization by simply seeing if the values of ***X*** and ***M*** have exactly the same order.

##Polychoric methodology

Similar to the polyserial, the polychoric is a simple case of two continuous variables ***X*** and ***Y*** that have a bivariate normal distribution. In the case of the polyserial the continious (latent) varialbe ***Y*** was observed as a discretized variable ***M***. For the polychoric this is again true but now the continious (latent) variable ***X*** is observed as a discrete variable ***P*** according to

$$p_i= \begin{cases} 1 \quad \mathrm{if}  \theta'_2 < x_i < \theta'_3 \\ 2 \quad \mathrm{if} \theta'_3 < x_i < \theta'_4 \\ \vdots \\ t \quad \mathrm{if} \theta'_{t+1} < x_i < \theta'_{t+2} \end{cases}$$

where $\boldsymbol{\theta}$ remains the cut points for the distibution defining the transformation of ***Y*** to ***M*** and $\boldsymbol{\theta}'$ is the cut points for the tramsofmation from ***X*** to ***P***. Similar to $\boldsymbol{\theta}$, $\boldsymbol{\theta}'$ has $\theta_2' \equiv -\infty$ and $\theta_{t+2}' \equiv \infty$.

Similar to in the polyserial, $\mu_y$ is not identifed (or is irrelevant) because setting $\tilde{\mu}_y = \mu_y + a$ and $\tilde{\boldsymbol{\theta}}=\boldsymbol{\theta} + a$ lead to exactly the same values of $\mathbf{M}$ and so one of the two must be artibrary assigned. The same is true for $\mu_x$. A convenient decision is to decide $\mu_y = \mu_x \equiv 0$. A similar argument holds for $\sigma_y$ and $\sigma_x$ so that  $\sigma_y = \sigma_x \equiv 1$

Then the probability of any given $m_i$, $p_i$ pair is 

$$\mathrm{Pr}\left( \rho=r ; P=p_i, M=m_i \right) = \int_{\theta_{p_i+1}'}^{\theta_{p_i+2}} dx \int_{\theta_{m_i+1}}^{\theta_{m_i+2}} \mkern-40mu dy f(x,y|\rho=r)$$

where $\rho$ is the correlation coefficient.

Using this function as an objective, the log-likelihood is then maximized.
$$\ell(\rho;p,m) = \sum_i w_i \ln\left[\mathrm{Pr}\left( \rho=r ; P=p_i, M=m_i \right) \right] $$

###Polychoric computation
This again mirrors the treatment of the polyserial. The derivatives of $\ell$ can be computed but are not readily computed and so when the `ML` argumet is set to `FALSE` (the default) a one dimensional optimization of $\rho$ is calculated using `stats::optimize`. When the `ML` argument is set to `TRUE` a multi-dimensional optimization is done for $\rho$, $\boldsymbol{\theta}$, and $\boldsymbol{\theta}'$ using `minqa::bobyqa`. As is shown below, the difference between these two is slight, if present, and so the default value of `ML` is recomended.

Because the optimization is not perfect when the correlation is in a boundary condition ($\rho \in \{-1,1\}$), a check for perfect correlation  is performed before the above optimization by simply seeing if the values of ***P*** and ***M*** have a Goodman-Kruskal correlation coefficient of -1 or 1. When this is the case, the MLE of -1 or 1, respectivly, is returned.

# Correctness

It is easy to prove the consistency of the $\boldsymbol{\theta}$ for the polyserial and $\boldsymbol{\theta}$ and $\boldsymbol{\theta}'$ using the non-ML case. Similarly, for $\rho$, because it is an MLE that can be obtained by taking a derivative and setting it equal to zero, the results are asymtotically unbiased and obtain the Cramer-Rao lower bound.

This does not speak to the small sample properties of these correlation coefficients. Previous work has described their properties by simulation and so that tradition is continued below.

## General setup for the unweighted case

A simulation is run several times using the following method:

* selecting a value of $n$ (the number of observations)
* selecting a true correlation coefficient $\rho$
* generating ***X*** and ***Y*** 
* selecting the value of $t$ and $t'$ (the number of bins for ***M*** and ***P***)
* selecting $\boldsymbol{\theta}$ and $\boldsymbol{\theta}'$
* confirming that at least 2 levels of ***M*** and ***P*** are occupied (if not, retrun to generating ***X*** and ***Y***)
* calculating and recording relevant statistics

## bias of the correlations
This sections shows the bias of the correlations as a function of the true correlation coefficient, $\rho$. These simulations were carried out using (junk about true $\rho$ and $n$ grids and number of reps).

## RMSE of the correlations as a function of $n$ and $\rho$

This plot shows the bias as a function of the true correlation $\rho$

```{r, echo=FALSE}
bias$rmse <- sqrt( (bias$est - bias$rho)^2 )
bias$bias <- bias$est - bias$rho
agg <- summaryBy(bias + rmse  ~ n + rho + type, data=bias, FUN=mean, na.rm=TRUE)

xyplot(bias.mean ~ rho|type,
      data=agg,
      groups=n,
      type="l",
      ylab="bias",
      xlab=expression(rho),
      auto.key=list(lines=TRUE, points=FALSE),
      par.settings=list(superpose.line=list(lwd=2), plot.line=list(lwd=2)))
```

this plot shows the RMSE as a function of $\rho$

```{r, echo=FALSE}
xyplot(rmse.mean ~ rho|type,
      data=agg,
      groups=n,
      scales=list(y=list(log=10)),
      ylab="RMSE",
      xlab=expression(rho),
      type="l",
      auto.key=list(lines=TRUE, points=FALSE),
      par.settings=list(superpose.line=list(lwd=2), plot.line=list(lwd=2)))
```


This plot shows the RMSE as a function of $n$

```{r, echo=FALSE}
agg <- summaryBy(rmse  ~ n+type, data=bias, FUN=mean, na.rm=TRUE)
xyplot(rmse.mean ~ n,
     groups=type,
      data=agg,
      ylab="RMSE",
      xlab="n",
      scales=list(y=list(log=10), x=list(log=10)),
      type="l",
      auto.key=list(lines=TRUE, points=FALSE),
      par.settings=list(superpose.line=list(lwd=2), plot.line=list(lwd=2)))
```


This plot shows the mean time to compute a single correlation coefficient as a function of $\rho$ by $n$ size.

```{r, echo=FALSE}
agg <- summaryBy(t ~ n + type, data=ntime, FUN=mean, na.rm=TRUE)
agg$t.mean <- ifelse(agg$t.mean==0, 0.001,agg$t.mean)

xyplot(t.mean ~ n|type,
       data=agg,
       scales=list(y=list(log=10), x=list(log=10)),
       type="l",
       ylab="compute time",
       xlab="n",
       auto.key=list(lines=TRUE, points=FALSE),
       par.settings=list(superpose.line=list(lwd=2), plot.line=list(lwd=2)))
```

## General setup for the weighted case
For the weighted case the unweighted result would be consistent if there were not something about the higher weight cases that makes them diffeent from the lower weight cases. Thus, while it is not reasonable to always assume that thee is a difference between the high and low weight cases, boht are possible and it serves as a more robust test of the methods in this package to consider a cases where they are associated.

There are many possible ways to associate the higher and lower weight cases. For the purposes of this document it is assumed that there is a third variable ***W*** that is correlated with ***X*** and ***Y*** and that the varible ***W*** is also the weight. While in a pratical cases it is unlikely that the weight itself would be causing changes in the relationship between ***X*** and ***Y***, this is a simplifying assumption that is used as a conveniance--averting the need for a fourth variable. 

Thus, the general setup is this:

$$\begin{pmatrix} \mathbf{X} \\ \mathbf{Y} \\ \mathbf{W} \end{pmatrix} \sim N \left[ \begin{pmatrix} \mu_x \\ \mu_y \\ \mu_w \end{pmatrix}, \Sigma \right] $$

for conveniance, and without loss of generality, it is assumed that $\mu_x = \mu_y = \mu_w \equiv 0$ and that

$$\Sigma \equiv \begin{pmatrix} 1 & \rho & a \\ \rho & 1 & b \\ a & b &  1 \end{pmatrix} $$

where $\rho$ remails the parameter of interest and $a\in [0,1]$ and $b \in [0,1]$ are the correlation between ***X*** and ***W*** and the correlation between ***Y*** and ***W***, respectively.

Simulations are carred out in the same fashion as before, except that after $\rho$ is elected, $a$ and $b$ are aslo selected.

##Polyserial simulation study, weighted

* method(s) of selecting $n$, $\rho$, $\rho$, $a$, $b$, $t$, $t'$,  $\boldsymbol{\theta}$, $\boldsymbol{\theta}'$.

then make these plots (note: include Pearson and Spearman):
* Compare weighted and unweighted bias as a function of $\rho$ and $a=b$, all $n=$ something reasonable based on the final bias chart . 
* Compare weighted and unweighted RMSE as a function of $\rho$ and $a=b$, all $n=$ something reasonable based on the final bias chart. 

* Compare weighted and unweighted bias as a function of $\rho$ and $a$ while $b=0$, all $n=$ something reasonable based on the final bias chart. 
* Compare weighted and unweighted RMSE as a function of $\rho$ and $a$ while $b=0$, all $n=$ something reasonable based on the final bias chart. 

##Polychoric simulation study, weighted


